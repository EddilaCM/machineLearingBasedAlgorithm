## 1. MCMC
1. 为什么会有MCMC？



这就要说起贝叶斯公式了，贝叶斯的先验结果是一种专家经验，而贝叶斯给出的后验分布是贝叶斯推断的基本工具。**但我们碰到比较复杂的先验，用贝叶斯就会得到非常复杂的后验分布，计算上相当困难。**

思想：就是它的名字————**两个MC**

**第一个MC：**Monte Carlo（蒙特卡罗），用随机变量（随机抽样）来解决计算问题。意味着：后验分布作为一个随机样本生成器，我们利用它来生成样本，然后通过这些样本对一些感兴趣的问题进行估计。

**第二个MC：**Markov Chain(马尔科夫链)。因为第一个MC中利用后验分布生成随机样本，但后验分布太复杂，我们可以利用Markov Chain 的**平稳分布**实现对复杂后验分布的抽样。

2. 什么是Markov chain?


## 2. SIFT算子的检测
## 3. Gibbs sample
## 4. LDA(主题模型)
简单说，就是对一堆文档进行聚类（非监督学习），一个topic就是一个类，topic的数目需要事先指定，聚类的结果是一个**概率**。

一个文档可能包含N个主题，一个主题可能包含M个关键词

![](https://i.imgur.com/Ad84yjh.png)
## 5. 两个字符串的编辑距离
## 6. entropy
1.entropy的本质是信息量 **log（1/P）=-log(P)**的期望。他是一个随机变量确定性的度量。熵越大，变量的取值越不确定，反之就越确定。【变量的每一个取值都会对应一个信息量。】

一个事件发生概率越大，它所携带的信息量就越小。

![](https://i.imgur.com/eJB1kNn.jpg)


某种意义上讲，信息量就是**使这个事件必然发生需要额外添加一下条件或者属性，这些条件或者属性的总量就是信息量**

![](https://i.imgur.com/XFySuMY.gif)

举例：

	假设： 
	 事件A：小明考试及格，对应的概率P(xA)=0.1，信息量为I(xA)=−log(0.1)=3.3219 
	 事件B：小王考试及格，对应的概率P(xB)=0.999，信息量为I(xB)=−log(0.999)=0.0014 
	可以看出，结果非常符合直观：小明及格的可能性很低(十次考试只有一次及格)，因此如果某次考试及格了（大家都会说：XXX竟然及格了！），必然会引入较大的信息量，对应的I值也较高。而对于小王而言，考试及格是大概率事件，在事件B发生前，大家普遍认为事件B的发生几乎是确定的，因此当某次考试小王及格这个事件发生时并不会引入太多的信息量，相应的I值也非常的低。
熵的单位随着公式中log运算的底数而变化，当底数为2时，单位为“比特”(bit)，底数为e时，单位为“奈特”。

3.相对熵，又称为KL散度（Kullback Leibler divergence）。KL距离就是两个随机分布间距离的度量。

![](https://i.imgur.com/n1SRtWB.jpg)

**Hp（q）**表示在P分布下，使用q进行编码需要的bit数

**H(P)**表示对真实分布P所需要的最小编码Bit数

**【编码就是log 取底数为2时的熵值。】**

相对熵就是指用q分布进行编码与用真实分布P进行编码所多出来的bit数【有点（估计量-真实值）的意思。】
4.交叉熵

![](https://i.imgur.com/YaiqnZW.jpg)

交叉熵与相对熵就仅仅多了一个H(p)，当P一致的时候，H(p)看作一个常数，此时交叉熵与KL距离在行为上是等价的，都反应了p,q的相似程度。

最小化交叉熵等于最小化KL距离。