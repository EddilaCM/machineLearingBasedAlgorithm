#算法名称：C4.5

决策树算法主要包含：ID3(分类树)、C4.5（分类树）、CART（回归树）

##1. 为什么会有这个算法？它是解决什么问题的？
C4.5是决策树算法的一种，它是ID3的一种改进算法，其实与ID3的主要区别在于***如何选择节点？***
<span style="border-bottom:1px solid #ccc">ID3在节点的选择（节点的选择也叫做分裂规则）时，利用的是信息增益；C4.5则用的是gain ratio（信息增益率）。</span>既然是决策树，那自然就是解决分类问题啦，是一种监督学习，它的目标是通过学习，找到从属性值到类别的映射关系，并用这个关系对新的不知道类别的实体进行分类。

但是，**C4.5并不是一个算法，而是一组算法！！！**
##2. 算法的原理是什么？
ID3：**信息增益** = **加入这个节点后的信息熵** - **没有加入前的信息熵**

C4.5：**信息增益率** = **加入这个点后的信息增益** /**没有加入前的信息熵**
##3. 算法的步骤是什么？
**递归地构造决策树**

1. 如果节点满足停止分裂条件，将其置为叶子节点。
2. 选择**信息增益率**最大的特征进行分裂。
3. 重复1、2，直到分类完成。

	ID3的过程只是在第二部的时候选择用“**信息增益**”最大的特征进行分裂。


##4. 算法的优缺点是什么？
**决策树本身的优势**在于不需要任何领域的知识或者参数设置，利用数据本身进行学习！

基于特征属性的分类的优点：模型的可读性强，准确率高。

<span style=" background:red;color:white;">C4.5利用信息增益率在分裂时进行属性选择，克服了信息增益选择属性时，变相选择取值多的属性的不足。</span>

缺点：

（1）构造过程中，需要对数据集进行多次顺序扫描和排序，因而导致算法低效；只适合驻留内存的数据集，当训练集大的无法在内存容纳时，程序无法运行。

（2）对未知数据预测不准确。即测试集中出现训练集中没有出现的属性。

=====解决===> 计算信息增益率->信息增益->**概率计算时，分子分母进行修正**

（3）发生过拟合。

发生过拟合的根本原因是模型过于复杂，可能的原因是：（a）训练集中有噪音样本点（b）叶子节点中缺乏有分类价值的样本记录，这时叶子节点应该被剪掉。

=====解决===>**剪枝（先剪枝和后剪枝）**

通常，剪枝都是基于统计，剪去不可靠的分枝。

先剪枝：提前停止树的构造。

	方法：
	（1）树达到一定高度后，不再继续构建树。
    （2）到达此处的节点具有相同特征向量，而且不属于同一类，可以停止生长。
	（3）实例个数达到某个阈值时，也可以停止生长。	
	 (4)计算每一次拓展的整个模型的增益，达到一定阈值可以停止生长。

**先剪枝的缺点**：出现这样的问题——当前拓展满足停止条件，进一步拓展不满足停止条件。

后剪枝：树构建好后，按照一定规则剪枝。

	删除节点的分支，并用树叶来替换。

**C4.5采用悲观剪枝（pessimistic Error pruning ,PEP）的方法**

悲观剪枝的基本思路：把一颗子树用一个叶子节点替代。