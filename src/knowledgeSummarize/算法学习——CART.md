#算法名称：CART（classification and regression Tree，分类回归树）
决策树主要有两类：

1. 分类树——输出是类别
2. 回归树——输出一个具体的实数

分类回归树是一棵二叉树，且每个非叶子节点都有两个孩子，其子树的叶子节点会比非叶子节点多1。
##1. 为什么会有这个算法？它是解决什么问题的？
情景一：实际生活中，往往会出现一个实例可能具有两个及其以上的类别，那么这时候进行分类问题处理，ID3和C4.5就局限了，CART就能够考虑将目标类别合并成两个超类别（双化）；

情境二：实例的属性的取值是连续，当然我们可以进行离散化操作后再进行分类，但是***如何用树解决回归问题呢？***这时，CART算法能够找出**一组基于树的回归方程**来**预测目标变量**。

所以，<span style="color:red">CART就能够用决策树来解决多分类问题和回归问题。</span>
##2. 算法的原理是什么？
**自上而下地构建树**，*怎么来构建树呢？*同样是用到属性的分裂规则，那么，
*怎么来分裂呢？*

在每一步都选着一个最好的属性来分裂。

**最好的属性是什么？**就是使得子节点中的训练集尽量的纯。不同的算法使用不同的指标来定义“最好”。

纯度的度量如下：

1. 对于分类目标变量，可以(a). **GINI**;(b). **双化或者有序化**；
2. 对于回归目标变量，可以使用(a). **最小二乘偏差**[LSD]; (b).**最小绝对值偏差**[LAD] 

选择变量的不纯性度量——**Gini指标**

Gini指数——紊乱的程度（正比）：

1. 一种不等性度量。
2. 通常用来度量收入不平衡，可以用来度量任何不均匀分布。
3. 介于0—~1之间的数，0——完全相等，1——完全不相等。
4. 总体会包含的内部越杂乱，Gini指数就越大（跟熵的概念很相似）

在CART算法中，基尼不纯度**表示一个随机选种的样本在子集中被分错的概率**。当一个节点中所有样本都是一个类时，基尼不纯度为0。

基尼指数的计算公式如下：

![](https://i.imgur.com/UGI83Hs.jpg)

 fi——表示y所有可能取值的概率

**举例**

	比如，y表示一个属性（体温）的取值恒温和非恒温{因为CART是一个完全二叉树，只会出现两个分支}，对应我们的分类结果（哺乳动物、鸟类等的体温都是恒温的）

	假设恒温包含哺乳动物5个，鸟类2个：Gini=1-【（5/7）^2+(2/7)^2】=20/49

	假设非恒温包含爬行类3个，鱼类3个，两栖类2个：Gini=1-【(3/8)^2+(3/8)^2+(2/8)^2】=42/63

	所以按照“体温为恒温和非恒温”进行划分，Gini增益：Gini_gain=7/15*20/49+8/15*42/64

##3. 算法的步骤是什么？
1. 从根节点t=1开始，从所有可能的候选集S集合中搜索使不纯度性降低最大的划分S*,使用划分的S*将节点1（t=1时是根节点）划分成两个节点t=2和t=3;
2. 在 t=2和t=3上分别重复划分搜索过程。
##4. 算法的优缺点是什么？

优点：

（1）非常灵活，允许有部分错分；可以指定先验概率分布，可以使用自动的成本复杂性剪枝来得到归纳性更强的树。
   
（2）面对有缺失值、变量数多等问题时，CART树显得非常稳健。